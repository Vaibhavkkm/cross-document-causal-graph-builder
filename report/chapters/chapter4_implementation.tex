\chapter{Implementation}
\label{ch:implementation}

\section{Development Environment}

The project was developed using the following technologies. The complete source code is available in the project repository \cite{mangroliya2024causal}.

\begin{table}[H]
    \centering
    \caption{Development Environment}
    \label{tab:dev-env}
    \begin{tabular}{ll}
        \toprule
        \textbf{Component} & \textbf{Version/Details} \\
        \midrule
        Programming Language & Python 3.7+ \\
        NLP Library & Hugging Face Transformers \\
        Graph Library & NetworkX \\
        Visualization & Matplotlib \\
        Deep Learning Framework & PyTorch \\
        Data Format & JSON \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Project Structure}

The project is organized as follows:

\begin{lstlisting}[language=bash, caption={Project Directory Structure}]
project/
    data/
        processed_data.json       # Preprocessed documents
    final_dataset/
        *.txt                     # 1,490 historical documents
    src/
        extract_rulebased.py      # Rule-based extraction
        extract_ml.py             # Hybrid ML extraction
        generate_cause_effect_network.py  # Visualization
    output/
        cause_effect_rulebased.json  # Rule-based results
        cause_effect_ml.json         # ML-based results
        network_rulebased.png        # Rule-based visualization
        network_ml.png               # ML-based visualization
        mapping_rulebased.txt        # Event labels
        mapping_ml.txt               # Event labels
    requirements.txt
    README.md
\end{lstlisting}

\section{Rule-Based Extractor Implementation}

\subsection{CauseEffectDetector Class}

The rule-based extraction is implemented in the \texttt{CauseEffectDetector} class. The class encapsulates all functionality for pattern matching, entity extraction, and relationship validation.

\begin{lstlisting}[language=Python, caption={CauseEffectDetector Initialization}]
class CauseEffectDetector:
    def __init__(self, min_conf=0.85):
        self.min_conf = min_conf
        
        # Causal phrases for pattern matching
        self.causal_phrases = {
            'forward': [
                r'\b(caused|led to|resulted in|triggered)\b',
                r'\b(consequently|therefore|thus|hence)\b',
                r'\b(in response to|following|after the)\b',
            ],
            'reverse': [
                r'\b(because|due to|owing to)\b',
                r'\b(was caused by|resulted from)\b',
            ]
        }
        
        # WWI-specific entities
        self.important_events = {
            'somme', 'verdun', 'ypres', 'gallipoli',
            'france', 'belgium', 'germany', 'british',
            'battalion', 'artillery', 'infantry'
        }
\end{lstlisting}

\subsection{IDF Building and TF-IDF Computation}

\begin{lstlisting}[language=Python, caption={TF-IDF Implementation}]
def build_idf(self, docs):
    """Build IDF scores from document collection"""
    self.ndocs = len(docs)
    freq = defaultdict(int)
    for doc in docs:
        for w in set(self.tokenize(doc)):
            freq[w] += 1
    for w, f in freq.items():
        self.idf[w] = math.log(self.ndocs / (1 + f))

def tfidf(self, text):
    """Compute TF-IDF vector for a sentence"""
    words = self.tokenize(text)
    if not words:
        return {}
    tf = Counter(words)
    mx = max(tf.values())
    return {w: (c/mx) * self.idf.get(w, 0) 
            for w, c in tf.items()}
\end{lstlisting}

\subsection{Cosine Similarity Computation}

\begin{lstlisting}[language=Python, caption={Cosine Similarity Function}]
def cosine(self, v1, v2):
    """Compute cosine similarity between TF-IDF vectors"""
    if not v1 or not v2:
        return 0.0
    common = set(v1) & set(v2)
    dot = sum(v1[w] * v2[w] for w in common)
    m1 = math.sqrt(sum(x**2 for x in v1.values()))
    m2 = math.sqrt(sum(x**2 for x in v2.values()))
    return dot / (m1 * m2) if m1 > 0 and m2 > 0 else 0.0
\end{lstlisting}

\subsection{Causal Pattern Detection}

\begin{lstlisting}[language=Python, caption={Causal Language Detection}]
def has_causal(self, text):
    """Check for causal phrases in text"""
    tl = text.lower()
    for direction, patterns in self.causal_phrases.items():
        for pattern in patterns:
            match = re.search(pattern, tl)
            if match:
                return True, direction, match.group(0)
    return False, '', ''
\end{lstlisting}

\subsection{Entity Extraction}

\begin{lstlisting}[language=Python, caption={Entity Extraction Function}]
def get_entities(self, text):
    """Extract named entities from text"""
    entities = set()
    text_lower = text.lower()
    
    # Check for known WWI entities
    for event in self.important_events:
        if event in text_lower:
            entities.add(event)
    
    # Extract dates (Month Year format)
    date_pattern = r'\b(january|february|march|...)\s+\d{4}\b'
    for match in re.findall(date_pattern, text_lower):
        entities.add(match)
    
    # Extract proper nouns (capitalized words)
    for match in re.findall(r'\b([A-Z][a-z]{3,})\b', text):
        if match.lower() not in self.excluded:
            entities.add(match.lower())
    
    return entities
\end{lstlisting}

\subsection{Validation Algorithm}

\begin{algorithm}[H]
\caption{Causal Pair Validation}
\label{alg:validation}
\begin{algorithmic}[1]
\Require Cause text $C$, Effect text $E$, Cause file $F_C$, Effect file $F_E$
\Ensure Validity flag, Confidence score, Details

\If{$F_C = F_E$}
    \State \Return False, 0, ``Same file''
\EndIf

\If{$|C| < 50$ \textbf{or} $|E| < 50$}
    \State \Return False, 0, ``Too short''
\EndIf

\State $has\_causal\_C, dir\_C \gets$ \Call{HasCausal}{$C$}
\State $has\_causal\_E, dir\_E \gets$ \Call{HasCausal}{$E$}

\If{\textbf{not} $has\_causal\_C$ \textbf{and not} $has\_causal\_E$}
    \State \Return False, 0, ``No causal language''
\EndIf

\State $sim \gets$ \Call{Cosine}{TFIDF($C$), TFIDF($E$)}

\If{$sim < 0.15$ \textbf{or} $sim > 0.65$}
    \State \Return False, 0, ``Invalid similarity''
\EndIf

\State $entities\_C \gets$ \Call{GetEntities}{$C$}
\State $entities\_E \gets$ \Call{GetEntities}{$E$}
\State $shared \gets entities\_C \cap entities\_E$

\If{$|shared| < 2$}
    \State \Return False, 0, ``Insufficient shared entities''
\EndIf

\State $score \gets$ \Call{ComputeConfidence}{...}

\State \Return $score \geq min\_conf$, $score$, details
\end{algorithmic}
\end{algorithm}

\section{Hybrid ML Extractor Implementation}

\subsection{HybridCauseEffect Class}

\begin{lstlisting}[language=Python, caption={Hybrid Extractor Initialization}]
class HybridCauseEffect:
    def __init__(self, min_conf=0.85):
        self.min_conf = min_conf
        
        print("Loading NLI model...")
        self.nli = pipeline(
            "zero-shot-classification",
            model="valhalla/distilbart-mnli-12-3",
            device=-1  # CPU
        )
        print("Model loaded.")
        
        # Same causal patterns as rule-based
        self.causal_phrases = {...}
        self.important_events = {...}
\end{lstlisting}

\subsection{ML Score Computation}

\begin{lstlisting}[language=Python, caption={NLI-Based Causal Scoring}]
def ml_score(self, cause, effect):
    """Calculate NLI entailment score"""
    # Format input for causal assessment
    text = f"{cause[:150]}. As a result, {effect[:120]}"
    
    # Zero-shot classification
    result = self.nli(
        text,
        candidate_labels=["causal relationship", "unrelated"]
    )
    
    # Extract probability for causal relationship
    idx = result['labels'].index('causal relationship')
    return result['scores'][idx]
\end{lstlisting}

\subsection{Combined Scoring}

\begin{lstlisting}[language=Python, caption={Combined Score Calculation}]
# Add ML scores to rule-filtered pairs
for pair in results:
    pair['ml_score'] = round(
        self.ml_score(pair['cause_text'], pair['effect_text']),
        4
    )
    pair['combined_score'] = round(
        (pair['rule_score'] + pair['ml_score']) / 2,
        4
    )

# Sort by combined score
results.sort(key=lambda x: x['combined_score'], reverse=True)
\end{lstlisting}

\section{Network Visualization Implementation}

The network visualization uses the NetworkX library \cite{hagberg2008exploring} for graph construction and analysis.

\subsection{Graph Construction}

\begin{lstlisting}[language=Python, caption={Network Graph Construction}]
def generate_network(input_json, output_png, output_txt, title):
    with open(input_json, 'r') as f:
        data = json.load(f)
    
    G = nx.DiGraph()
    events = {}
    counter = 1
    
    for rel in data:
        cause_key = (rel['cause_file'], rel['cause_text'][:100])
        effect_key = (rel['effect_file'], rel['effect_text'][:100])
        
        # Create unique node IDs
        if cause_key not in events:
            cid = f"C{counter}"
            events[cause_key] = {
                'id': cid, 
                'file': rel['cause_file'],
                'text': rel['cause_text']
            }
            counter += 1
        
        # Add nodes and edges to graph
        G.add_node(cid, type='cause')
        G.add_node(eid, type='effect')
        G.add_edge(cid, eid, confidence=conf)
\end{lstlisting}

\subsection{Visualization Rendering}

\begin{lstlisting}[language=Python, caption={Graph Visualization}]
# Create figure
fig, ax = plt.subplots(figsize=(20, 16))
pos = nx.spring_layout(G, k=0.5, iterations=100, seed=42)

# Draw edges (causal links)
nx.draw_networkx_edges(
    G, pos,
    edge_color='red',
    alpha=0.5,
    arrows=True,
    arrowsize=12,
    style='dashed'
)

# Draw cause nodes (light blue)
nx.draw_networkx_nodes(
    G, pos,
    nodelist=causes,
    node_color='#87CEEB',
    node_size=500
)

# Draw effect nodes (light green)
nx.draw_networkx_nodes(
    G, pos,
    nodelist=effects,
    node_color='#90EE90',
    node_size=500
)

plt.savefig(output_png, dpi=300, bbox_inches='tight')
\end{lstlisting}

\section{Output Format}

\subsection{Rule-Based Output JSON}

\begin{lstlisting}[language=json, caption={Rule-Based Output Format}]
{
  "type": "cross_file",
  "cause_file": "survivor_008.txt",
  "cause_text": "When German infantry advanced...",
  "effect_file": "survivor_042.txt",
  "effect_text": "Intense fighting led to losses...",
  "confidence": 1.0
}
\end{lstlisting}

\subsection{ML-Based Output JSON}

\begin{lstlisting}[language=json, caption={ML-Based Output Format}]
{
  "cause_file": "survivor_044.txt",
  "cause_text": "Walter Greenwood remembered...",
  "effect_file": "history_015.txt",
  "effect_text": "Britain fearing German domination...",
  "rule_score": 0.95,
  "ml_score": 0.98,
  "combined_score": 0.97,
  "shared_context": ["britain", "germany", "german"]
}
\end{lstlisting}

\section{Execution Pipeline}

\subsection{Running Rule-Based Extraction}

\begin{lstlisting}[language=bash]
cd src
python3 extract_rulebased.py
\end{lstlisting}

\subsection{Running ML-Based Extraction}

\begin{lstlisting}[language=bash]
cd src
python3 extract_ml.py
\end{lstlisting}

\subsection{Generating Visualizations}

\begin{lstlisting}[language=bash]
cd src
python3 generate_cause_effect_network.py both
\end{lstlisting}

\section{Summary}

This chapter has detailed the implementation of the causal extraction system. The rule-based extractor uses linguistic patterns and semantic similarity, while the hybrid ML extractor adds transformer-based validation using the Hugging Face Transformers library \cite{wolf2019huggingface}. Both produce structured JSON output and network visualizations. The following chapter presents the results and analysis.
