\chapter{Appendix}
\label{ch:appendix}

\section{Source Code Repository}

The complete source code for this project is available on GitHub:

\begin{center}
\fbox{\parbox{0.8\textwidth}{
\centering
\textbf{GitHub Repository}\\[0.3cm]
\url{https://github.com/Vaibhavkkm/cross-document-causal-graph-builder}
}}
\end{center}

\noindent The repository contains:
\begin{itemize}
    \item \texttt{src/extract\_rulebased.py} -- Rule-based causal extraction
    \item \texttt{src/extract\_ml.py} -- Hybrid ML-based extraction
    \item \texttt{src/generate\_cause\_effect\_network.py} -- Network visualization
    \item \texttt{data/} -- Processed document data
    \item \texttt{output/} -- Extraction results and visualizations
    \item \texttt{final\_dataset/} -- WWI historical documents
\end{itemize}

\section{Sample Output Data}

\subsection{Rule-Based Output Sample}

\begin{lstlisting}[language=json, caption={Sample from cause\_effect\_rulebased.json}]
{
  "type": "cross_file",
  "cause_file": "survivor_008.txt",
  "cause_text": "When German infantry advanced in large 
    numbers during the battle, rapid British rifle fire 
    resulted in heavy losses.",
  "effect_file": "survivor_042.txt",
  "effect_text": "Intense fighting on that first day led 
    to further German gains and heavy British losses.",
  "confidence": 1.0
}
\end{lstlisting}

\subsection{ML-Based Output Sample}

\begin{lstlisting}[language=json, caption={Sample from cause\_effect\_ml.json}]
{
  "cause_file": "survivor_044.txt",
  "cause_text": "Walter Greenwood, a schoolboy in Britain 
    at the time, remembered the anger felt towards Germany.",
  "effect_file": "history_015.txt",
  "effect_text": "Britain fearing German domination of Europe 
    because if a victorious Germany dominated the continent.",
  "rule_score": 0.95,
  "shared_context": ["britain", "germany", "german"],
  "ml_score": 0.9826,
  "combined_score": 0.9663
}
\end{lstlisting}

\section{Sample Historical Documents}

\subsection{Personal Letter Example (bob\_001.txt)}

\begin{quote}
\textit{From: Bob Henderson, aboard a troopship}\\
\textit{Date: 21st August 1915}\\
\textit{To: Mrs. James Henderson, Sydney, NSW}

My Dear Mum \& family, Saturday 21st August 1915, Not much news for you yet but just a line to let you know everything is all right \& we are all a happy family. The food on board is excellent plenty of butter etc. Porridge, meat \& rolls for breakfast. Soup, meat and pudding for dinner \& cold meat pickles etc for tea...
\end{quote}

\subsection{Survivor Account Example (survivor\_001.txt)}

\begin{quote}
The Christmas Truce of 1914 remains one of the most remarkable events of World War I. On Christmas Eve, German soldiers began placing candles on their trenches and singing carols. British soldiers responded in kind, and by Christmas morning, soldiers from both sides emerged from their trenches to exchange greetings, share cigarettes and food, and even play football in No Man's Land...
\end{quote}

\section{Environment Setup}

\subsection{Requirements}

\begin{lstlisting}[language=bash, caption={requirements.txt}]
networkx>=2.5
matplotlib>=3.3.0
transformers>=4.0.0
torch>=1.7.0
\end{lstlisting}

\subsection{Installation}

\begin{lstlisting}[language=bash, caption={Setup Commands}]
# Clone repository
git clone https://github.com/Vaibhavkkm/cross-document-causal-graph-builder.git
cd cross-document-causal-graph-builder

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Run extraction
python src/extract_rulebased.py
python src/extract_ml.py
python src/generate_cause_effect_network.py both
\end{lstlisting}

\section{Glossary of Terms}

\begin{description}
    \item[TF-IDF] Term Frequency-Inverse Document Frequency, a statistical measure of word importance in a document relative to a corpus.
    \item[NLI] Natural Language Inference, the task of determining logical relationships (entailment, contradiction, neutral) between sentence pairs.
    \item[MNLI] Multi-Genre Natural Language Inference, a large-scale benchmark dataset for NLI.
    \item[DistilBART] A distilled (compressed) version of the BART transformer model, offering faster inference with minimal accuracy loss.
    \item[Cross-Document] Relationships that span multiple source documents, as opposed to within-document relationships.
    \item[Cosine Similarity] A measure of similarity between two vectors based on the cosine of the angle between them.
    \item[NetworkX] A Python library for creating, manipulating, and analyzing complex networks and graphs.
    \item[WWI] World War I, the global military conflict from 1914 to 1918.
\end{description}
